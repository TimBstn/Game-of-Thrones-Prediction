{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab2aeeb",
   "metadata": {},
   "source": [
    "<h1> Game of Thrones - Death Prediction</h1>\n",
    "\n",
    "In this Notebook, I am trying to predict which character is going to die (isAlive = 0) and who is staying alive (isAlive = 1). Please run all cells to see the final output. The chosen model and score are given in the last cell, so stay tuned!\n",
    "\n",
    "<h2> Scoring </h2>\n",
    "As it is a classification problem, we cannot just use R-Squared. To evaluate the outcomes of the model, we will use the AUC scoring method. Additionally, we will look at the confusion matrix to see which kind of errors occurred. The errors we are interested in are false negatives and positives, explained further in the grid below.\n",
    "\n",
    "~~~\n",
    "                                                 |\n",
    "  True Negative                                  |  False Positive\n",
    "  PREDICTED: Character Died   (isAlive = 0)      |  PREDICTED: Character Alive   (isAlive = 1)\n",
    "  ACTUAL:    Character Died   (isAlive = 0)      |  ACTUAL:    Character Died    (isAlive = 0)\n",
    "                                                 |\n",
    "-------------------------------------------------|---------------------------------------------------\n",
    "                                                 |\n",
    "  False Negative                                 |  True Positive\n",
    "  PREDICTED: Character Died   (isAlive = 0)      |  PREDICTED: Character Alive   (isAlive = 1)\n",
    "  ACTUAL:    Character Alive  (isAlive = 1)      |  ACTUAL:    Character Alive   (isAlive = 1)\n",
    "                                                 |  \n",
    "~~~\n",
    "\n",
    "If two models are the same in the AUC-score, I would prefer the model with less False Positives. For me as a supporter of the show, it is worse thinking the character is alive and then realizing he died in the end. The other way around is way more emotional, as thinking the character died and realizing he is alive gives you kind of a good feeling.\n",
    "\n",
    "\n",
    "<h2> Preparing the data </h2>\n",
    "\n",
    "As usual, the first step in building a model is preparing the data.\n",
    "\n",
    "<h3> Gender guesser </h3>\n",
    "\n",
    "The first column I have imputed is one to guess the gender. For that, i split the name in first and last name at the first space. As the guessing process of the first name ran for more than 5 minutes, I decided to copy the outcome and only keep the final list. The gender guesser gave me the following outcome:\n",
    "\n",
    "~~~\n",
    "unknown          1385\n",
    "male              381\n",
    "female            125\n",
    "mostly_male        24\n",
    "mostly_female      21\n",
    "andy               10\n",
    "~~~\n",
    "\n",
    "Seeing that, I assume there are not a lot of English names in there and the value of the gender for the model will not be very high.\n",
    "\n",
    "<h3> Imputing missing values </h3>\n",
    "\n",
    "As always, imputing missing values took an important place in the data engineering part.\n",
    "\n",
    "<h4> Categorical columns </h4>\n",
    "First, I looked at the categorical columns, namely title, culture, mother, father, heir, house and spouse. I flagged the missing values in these columns in a new column and imputed the NaN's with <em>unknown</em>, as it would not make sense imputing missing values in these columns, because in the most cases no value in house or culture means the character is no part of such. \n",
    "\n",
    "<h4> Numerical columns </h4>\n",
    "For the numerical columns, namely age and date of birth, I decided to impute with the median. As we had two outliers that were very big (positive for date of birth, negative for age), it was no option to impute with the mean (the age would have come out negative).\n",
    "\n",
    "<h4> Boolean columns </h4>\n",
    "For the boolean columns, namely the the check if mother, father, spouse or heir are alive, I decided to impute with 0. When one of these is NaN, it means the person is unknown, and therefor we cannot assume that the person is alive. \n",
    "\n",
    "<h3> Engineering new features </h3>\n",
    "Looking at the data, a couple of new columns came to my mind. First, as we have the outliers in age and year of birth, I decided to add both of these values together. Next, I created a column checking the size of the house a person is living in. Additionally, I looked at the number of books a person is in. When that number is zero, I assumed the person is not named in one of the books and therefor is in the TV show only. \n",
    "\n",
    "<h3> Creating Dummy-variables </h3>\n",
    "For the categorical columns gender, culture and house I created dummy variables. I decided not to do so for mother, father, spouse and heir, as these are too specific. For the culture column, I first grouped together cultures that are the same but only differ in the spelling, for example Summer Islands, Summer Islander and Summer Isles are all Summer Islands for me. \n",
    "\n",
    "\n",
    "<h3> Creating the final DataFrame </h3>\n",
    "Last, I separated the target variable <em> isAlive </em> from the rest.  I dropped the categorical columns and columns I do not use later on, for example the index column. Additionally, I dropped one column per dummy-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab811b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np                                      # mathematical essentials\n",
    "import pandas as pd                                     # data science essentials\n",
    "import matplotlib.pyplot as plt                         # data visualization\n",
    "import gender_guesser.detector as gender                # gender guessing\n",
    "\n",
    "# importing libraries for modeling\n",
    "import statsmodels.formula.api as smf                   # linear modeling\n",
    "from sklearn.model_selection import train_test_split    # training and testing\n",
    "from sklearn.metrics import roc_auc_score               #scoring metric\n",
    "from sklearn.metrics import make_scorer                 # customizable scorer\n",
    "from sklearn.metrics import confusion_matrix            # confusion matrix\n",
    "from sklearn.preprocessing import StandardScaler        # standard scaler\n",
    "from sklearn.model_selection import RandomizedSearchCV  # hyperparameter tuning\n",
    "from sklearn.linear_model import LogisticRegression     # logistic regression\n",
    "from sklearn.neighbors import KNeighborsClassifier      # KNN for Classification\n",
    "from sklearn.tree import DecisionTreeClassifier         # Decision tree for Classification\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "\n",
    "# loading data\n",
    "got = pd.read_excel(io = 'GOT_character_predictions.xlsx')\n",
    "\n",
    "\n",
    "# getting first and last name --> https://stackoverflow.com/questions/51290134/using-pandas-how-do-i-split-based-on-the-first-space\n",
    "got[['first_name', 'last_name']] = got['name'].str.split(n=1, expand=True)\n",
    "\n",
    "#placeholder_lst = []\n",
    "#for name in got['first_name']:\n",
    "#    guess = gender.Detector().get_gender(name)\n",
    "#    placeholder_lst.append(guess)\n",
    "\n",
    "#guessing the gender --> gender list is finished placeholder list\n",
    "gender_list = ['unknown', 'unknown', 'andy', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'male', 'mostly_male', 'mostly_male', 'mostly_male', 'mostly_male', 'mostly_male', 'mostly_male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'female', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'male', 'andy', 'andy', 'unknown', 'andy', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'male', 'unknown', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'mostly_male', 'male', 'mostly_male', 'mostly_male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'mostly_male', 'unknown', 'unknown', 'male', 'female', 'andy', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'mostly_female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'andy', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'female', 'female', 'female', 'female', 'female', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'mostly_female', 'female', 'unknown', 'mostly_female', 'unknown', 'female', 'unknown', 'female', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'andy', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'female', 'female', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'female', 'male', 'male', 'male', 'female', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'male', 'male', 'male', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'andy', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'male', 'unknown', 'unknown', 'female', 'male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'female', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'female', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'female', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'female', 'female', 'female', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'mostly_female', 'mostly_female', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'male', 'male', 'male', 'male', 'unknown', 'female', 'female', 'female', 'unknown', 'mostly_male', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'female', 'male', 'female', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'female', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'unknown', 'male', 'unknown', 'unknown', 'mostly_female', 'male', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'male', 'male', 'female', 'mostly_female', 'female', 'mostly_female', 'mostly_female', 'mostly_female', 'mostly_female', 'mostly_female', 'mostly_female', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'female', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'female', 'unknown', 'female', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'female', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'mostly_male', 'unknown', 'female', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'female', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'mostly_male', 'male', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'female', 'female', 'female', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'male', 'male', 'andy', 'male', 'male', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'female', 'unknown', 'male', 'male', 'male', 'male', 'male', 'male', 'mostly_male', 'mostly_male', 'mostly_male', 'mostly_male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'male', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'mostly_female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'mostly_female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'female', 'unknown', 'female', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'andy', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'male', 'unknown', 'male', 'male', 'unknown', 'male', 'unknown', 'male', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'male', 'male', 'male', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'mostly_male', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'mostly_female', 'unknown', 'unknown', 'unknown', 'female', 'male', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'female', 'male', 'mostly_male', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'female', 'unknown', 'unknown', 'unknown', 'male', 'unknown', 'unknown']\n",
    "\n",
    "\n",
    "# converting list into a series\n",
    "got['gender_guess'] = pd.Series(gender_list)\n",
    "\n",
    "# categorical columns\n",
    "cat_cols =['title', 'culture','mother','father', 'heir', 'house', 'spouse']\n",
    "\n",
    "#flagging missing values, impute with unknown for categorical columns\n",
    "for col in got[cat_cols]:\n",
    "    if got[col].isnull().astype(int).sum() > 0:\n",
    "        got['m_'+col] = got[col].isnull().astype(int)\n",
    "        got[col] = got[col].fillna('unknown')\n",
    "        \n",
    "# imputing missing values for age and birth with median\n",
    "got['dateOfBirth'] = got['dateOfBirth'].fillna(got['dateOfBirth'].median()).round(3)\n",
    "got['age'] = got['age'].fillna(got['age'].median()).round(3)\n",
    "\n",
    "# imputing missing values for boolean columns\n",
    "got['isAliveMother'] = got['isAliveMother'].fillna(0)               \n",
    "got['isAliveFather'] = got['isAliveFather'].fillna(0)                \n",
    "got['isAliveHeir'] = got['isAliveHeir'].fillna(0)              \n",
    "got['isAliveSpouse'] = got['isAliveSpouse'].fillna(0)  \n",
    "\n",
    "# create column depending on age and birth to get rid of outlier\n",
    "got['age_birth'] = got['dateOfBirth']+got['age']\n",
    "        \n",
    "# get house size, death rate\n",
    "got['houseSize'] = got['house'].map(got['house'].value_counts()) \n",
    "\n",
    "# total books in\n",
    "got['total_books_in'] = got['book1_A_Game_Of_Thrones']+ got['book2_A_Clash_Of_Kings']+ got['book3_A_Storm_Of_Swords']+got['book4_A_Feast_For_Crows']+got['book5_A_Dance_with_Dragons']\n",
    "\n",
    "# total books in is zero --> did not appear in books --> only appeared in TV-Show\n",
    "got['show_only'] = pd.cut(got['total_books_in'], [-1, 0, 6], include_lowest=True, labels=[1,0]).astype(int)\n",
    "\n",
    "\n",
    "# mapping same cultures into one\n",
    "cult = {\n",
    "    'Summer Islands': ['Summer Islands', 'Summer Islander', 'Summer Isles'],\n",
    "    'Ghiscari': ['Ghiscari', 'Ghiscaricari',  'Ghis'],\n",
    "    'Asshai': [\"Asshai'i\", 'Asshai'],\n",
    "    'Crannogs': ['Crannogmen'],\n",
    "    'Astapori': ['Astapori'],\n",
    "    'Lysene': ['Lysene', 'Lyseni'],\n",
    "    'Braavosi': ['Braavosi', 'Braavos'],\n",
    "    'Dornish': ['Dornishmen', 'Dorne', 'Dornish'],\n",
    "    'Dothraki': ['Dothraki'],\n",
    "    'Myrish': ['Myr', 'Myrish', 'Myrmen'],\n",
    "    'Westermen': ['Westermen', 'Westerman', 'Westerlands', 'westermen'],\n",
    "    'Westerosi': ['Westeros', 'Westerosi'],\n",
    "    'Stormlander': ['Stormlands', 'Stormlander'],\n",
    "    'Northmen': ['The north', 'Northmen', 'northmen', 'Northern mountain clans'],\n",
    "    'Free Folk': ['Wildling', 'First Men', 'free folk', 'Free Folk', 'Free folk' ],\n",
    "    'Qartheen': ['Qartheen', 'Qarth'],\n",
    "    'Reach': ['The Reach', 'Reach', 'Reachmen'],\n",
    "    'Ironborn': ['Ironborn', 'Ironmen', 'ironborn'],\n",
    "    'Mereen': ['Meereen', 'Meereenese'],\n",
    "    'RiverLands': ['Riverlands', 'Rivermen'],\n",
    "    'Vale': ['Vale', 'Valemen', 'Vale mountain clans'],\n",
    "    'Valyrian' : ['Valyrian'],\n",
    "    'Pentoshi' : ['Pentoshi'],\n",
    "    'Tyrosh' : ['Tyroshi'],\n",
    "    'Unknown' : ['unknown'],\n",
    "    'Other' : ['Ibbenese', 'Astapor', 'Lhazarene', 'Rhoynar', 'Naathi', 'Norvoshi', 'Norvos', 'Wildlings',\n",
    "              'Sistermen', 'Lhazareen', 'Andal', 'Andals', 'Qohor']\n",
    "}\n",
    "\n",
    "got['Cult_new'] = \"\"\n",
    "\n",
    "for culture in cult:\n",
    "    got.loc[got.culture.isin(values=cult[culture]), 'Cult_new'] = culture\n",
    "\n",
    "# creating dummy variables for gender and culture\n",
    "one_hot_gender       = pd.get_dummies(got['gender_guess'], prefix='gender') \n",
    "one_hot_culture      = pd.get_dummies(got['Cult_new'], prefix='culture')\n",
    "one_hot_house     = pd.get_dummies(got['house'], prefix='house')\n",
    "got = got.join(other = [one_hot_gender, one_hot_culture, one_hot_house])    \n",
    "\n",
    "# replacing spaces by underscore in house_ columns\n",
    "got.columns = got.columns.str.replace(' ', '_')\n",
    "\n",
    "# getting target variable\n",
    "got_target = got.loc[:, 'isAlive']\n",
    "    \n",
    "    \n",
    "# dropping not needed columns\n",
    "got = got.drop(['S.No','name', 'last_name', 'first_name', 'title', 'culture', 'Cult_new',\n",
    "                'mother', 'father', 'heir', 'spouse', 'house', \n",
    "                'gender_guess', 'isAlive', 'gender_unknown', 'culture_Unknown', 'house_unknown'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4160aa9",
   "metadata": {},
   "source": [
    "<h2> Model 1 - Logistic Regression </h2>\n",
    "\n",
    "My first approach for solving the problem is a logistic regression model. \n",
    "\n",
    "<h3> Train-Test-Split function </h3>\n",
    "For later use in all the models, I created a function that does a test and train split and allows to standardize if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for train and test split and standardizing\n",
    "def test_train_standardize( x_data,\n",
    "                            y_data,\n",
    "                            standardize = False,\n",
    "                            pct_test    = 0.1,\n",
    "                            seed        = 219):\n",
    "    \"\"\"\n",
    "Creates train set and test set for given X and y data. Standardizes, if chosen.\n",
    "Outputs the x and y data for testing and training. \n",
    "Parameters:\n",
    "x_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the x data, default False\n",
    "pct_test      : test size for training and validation from (0,1), default 0.1\n",
    "seed          : random seed to be used in algorithm, default 219\n",
    "    \"\"\"\n",
    "    if standardize == True:\n",
    "        # optionally standardizing x_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(x_data)\n",
    "        x_scaled           = scaler.transform(x_data)\n",
    "        x_scaled_df        = pd.DataFrame(x_scaled)\n",
    "        x_data             = x_scaled_df\n",
    "\n",
    "    # train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size    = pct_test,\n",
    "                                                        random_state = seed, \n",
    "                                                        stratify     = y_data)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1ffce",
   "metadata": {},
   "source": [
    "<h3> Creating combinations with low p-values </h3>\n",
    "In the logistic regression model, p-values are important. Therefor, I started to look into the correlation of the columns to the target variable and built a candidate dictionary with the most promising columns. After that, I checked for p-values using the smf library and only kept the column combinations with low p-values for all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a dataframe with all columns and target variable to look for correlation and p-values\n",
    "# got_full_logreg = got.join(got_target)\n",
    "\n",
    "# print(abs(got_full_logreg.corr()['isAlive']).sort_values(ascending=False).head(n=10))\n",
    "\n",
    "candidate_dic = {\n",
    "    'top_columns_1' : ['popularity', 'age_birth', 'book4_A_Feast_For_Crows'],\n",
    "    'top_columns_2' :['popularity', 'age_birth', 'book4_A_Feast_For_Crows', 'show_only'],\n",
    "    'top_columns_3' :['popularity', 'age_birth', 'book4_A_Feast_For_Crows', 'book1_A_Game_Of_Thrones'],\n",
    "    'top_columns_4' : ['age_birth', 'book4_A_Feast_For_Crows', 'culture_Valyrian', 'book1_A_Game_Of_Thrones'],\n",
    "    'top_columns_5' : ['numDeadRelations', 'age_birth', 'book4_A_Feast_For_Crows'],\n",
    "    'top_columns_6' : ['show_only', 'age_birth', 'book1_A_Game_Of_Thrones', 'm_mother']\n",
    "}\n",
    "\n",
    "# # instantiating a logistic regression model object\n",
    "# logit_full = smf.logit(formula = \"\"\" isAlive ~\n",
    "#                                     book4_A_Feast_For_Crows +\n",
    "#                                     book1_A_Game_Of_Thrones +\n",
    "#                                     show_only +\n",
    "#                                     popularity\n",
    "#                                     \"\"\",\n",
    "#                                      data    = got_full_logreg)\n",
    "\n",
    "# # fitting the model object\n",
    "# logit_full = logit_full.fit()\n",
    "\n",
    "# # checking the results SUMMARY\n",
    "# logit_full. summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7aa37",
   "metadata": {},
   "source": [
    "<h3> Running Logistic Regression </h3>\n",
    "For later use, I wrote a function that runs logistic regression automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ee132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_logreg( x, \n",
    "                    y,\n",
    "                    columns        = candidate_dic['top_columns_3'],\n",
    "                    ran_gridsearch = False,\n",
    "                    solver         = 'lbfgs',\n",
    "                    C              = 1,\n",
    "                    warm_start     = False):\n",
    "    \"\"\"\n",
    "Runs a logistic regression on a dataset and target variable. Slizes down the \n",
    "explanatory variable by predifined columns first and solves the logistic\n",
    "regression with the predefined parameters after.\n",
    "x              : Whole explanatory dataset\n",
    "y              : Whole target variable \n",
    "columns        : Columns used in explanatory variable\n",
    "ran_gridsearch : Variable that clarifies, if grid search was run. In that case, the model is fit on the\n",
    "                whole dataset, else only on the train-dataset. \n",
    "solver         : Solver used in logistic regression\n",
    "C              : C used in logistic regression\n",
    "warm_start     : warm start for logistic regression, either True or False\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # slize the columns\n",
    "    x = x[columns]   \n",
    "    \n",
    "    # initiate logistic regression\n",
    "    lr = LogisticRegression(solver       = solver,\n",
    "                            C            = C,\n",
    "                            warm_start   = warm_start,\n",
    "                            random_state = 219,\n",
    "                            max_iter     = 1000\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # creating train and test sets\n",
    "    x_train, x_test, y_train, y_test = test_train_standardize(x, y)\n",
    "    \n",
    "    # FITTING the training data\n",
    "    if ran_gridsearch:\n",
    "        # ran cv gridsearch before, therefor fitting on whole dataset \n",
    "        lr_fit = lr.fit(x, y)\n",
    "    else:\n",
    "        # no cv before --> fitting on train data\n",
    "        lr_fit = lr.fit(x_train, y_train)\n",
    "    \n",
    "    # PREDICTING based on the testing set\n",
    "    lr_pred = lr_fit.predict(x_test)\n",
    "\n",
    "    # saving scoring data\n",
    "    lr_train_score = lr_fit.score(x_train, y_train).round(4) # accuracy\n",
    "    lr_test_score  = lr_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "    \n",
    "    # saving auc score\n",
    "    lr_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = lr_pred).round(decimals = 4)\n",
    "    \n",
    "    # creating confusion matrix\n",
    "    lr_tn, lr_fp, lr_fn, lr_tp = confusion_matrix(y_true = y_test, y_pred = lr_pred).ravel()\n",
    "    lr_conf_matrix = [lr_tn, lr_fp, lr_fn, lr_tp]\n",
    "      \n",
    "    return [lr_test_score, lr_train_score, lr_auc_score, lr_conf_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd34c6",
   "metadata": {},
   "source": [
    "<h4> Testing column combinations </h4>\n",
    "\n",
    "The next step is to run this function on our pre-defined column sets and see which set performs best.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "top_columns_1      0.8103       0.8024     0.6366  [14, 36, 1, 144]\n",
    "top_columns_2      0.8103       0.8007     0.6366  [14, 36, 1, 144]\n",
    "top_columns_3      0.8564       0.8127     0.7397  [25, 25, 3, 142]\n",
    "top_columns_4      0.8462       0.7887     0.7262  [24, 26, 4, 141]\n",
    "top_columns_5      0.8103       0.7990     0.6300  [13, 37, 0, 145]\n",
    "top_columns_6      0.8103       0.7978     0.6300  [13, 37, 0, 145]\n",
    "~~~\n",
    "\n",
    "Looking at the outcome, we see that the combinations 3 and 4 seem to work best, shifting a couple False Positives to True Negatives. So for further testing in logistic regression, I will continue with these two options. Interestingly, these are the two options with book 1 and book 4 in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8760af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # running logistic regression for all set of columns in candidate dic\n",
    "# scores = []\n",
    "# for x in candidate_dic:\n",
    "#     score = optimal_logreg(x = got, y = got_target, columns = candidate_dic[x])\n",
    "#     scores.append(score)\n",
    "\n",
    "# lr_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= candidate_dic.keys())\n",
    "# lr_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82d157",
   "metadata": {},
   "source": [
    "<h4> Running Hyperparameter Tuning </h4>\n",
    "For the best models, in that case for the column sets 3 and 4, I will run a hyperparameter tuning to evaluate, if I can higher the AUC score a little bit. For that, I wrote a function. Running these, the parameters used for the columns should be:\n",
    "\n",
    "~~~\n",
    "               warm_start     solver    C\n",
    "top_columns_3       False  newton-cg  2.5\n",
    "top_columns_4        True  newton-cg  3.0\n",
    "~~~\n",
    "\n",
    "I then ran logistic regression on these two set of columns with their specified optimal hyperparameters and got the following output:\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "top_columns_3      0.8513       0.8007     0.7428  [26, 24, 5, 140]\n",
    "top_columns_4      0.8462       0.7893     0.7262  [24, 26, 4, 141]\n",
    "~~~\n",
    "\n",
    "For the column set 3, the score got up a little bit, while for set 3 it stayed unchanged. The best model built with Logistic Regression is therefor the one on column set 3 with tuned hyperparamaters scoring an AUC score of 0.7428."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression hyperparameter tuning\n",
    "def run_cv_logreg(x, \n",
    "                  y, \n",
    "                  columns = candidate_dic['top_columns_3']):\n",
    "    \"\"\"\n",
    "Runs a cross validation on logistic regression. Outputs the best parameters\n",
    "found in the search.\n",
    "Parameters:\n",
    "x       : Whole explanatory dataset\n",
    "y       : Whole target variable \n",
    "columns : columns used in explanatory variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # slize the columns\n",
    "    x = x[columns]  \n",
    "    \n",
    "    # declaring a hyperparameter space\n",
    "    C_range          = np.arange(0.1, 5.0, 0.1)\n",
    "    warm_start_range = [True, False]\n",
    "    solver_range     = ['newton-cg', 'sag', 'lbfgs']\n",
    "\n",
    "    # creating a hyperparameter grid\n",
    "    param_grid = {'C'          : C_range,\n",
    "                  'warm_start' : warm_start_range,\n",
    "                  'solver'     : solver_range}\n",
    "\n",
    "    # INSTANTIATING the model object without hyperparameters\n",
    "    lr_tuned = LogisticRegression(random_state = 219,\n",
    "                                  max_iter     = 1000) # increased for convergence\n",
    "\n",
    "    # GridSearchCV object\n",
    "    lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "                                     param_distributions = param_grid, # parameters to tune\n",
    "                                     cv                  = 3,          # how many folds in cross-validation\n",
    "                                     n_iter              = 250,        # number of combinations of hyperparameters to try\n",
    "                                     random_state        = 219,        # starting point for random sequence\n",
    "                                     scoring = make_scorer(\n",
    "                                               roc_auc_score,\n",
    "                                               needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "    # FITTING to the FULL DATASET (due to cross-validation)\n",
    "    lr_tuned_cv.fit(x, y)\n",
    "    \n",
    "    return lr_tuned_cv.best_params_\n",
    "\n",
    "# # find optimal hyperparameters for sets 3 and 4\n",
    "# opt_1 = run_cv_logreg(got, got_target, columns = candidate_dic['top_columns_3'])\n",
    "# opt_2 = run_cv_logreg(got, got_target, columns = candidate_dic['top_columns_4'])\n",
    "\n",
    "# df = pd.DataFrame([opt_1, opt_2], index = ['top_columns_3', 'top_columns_4'])\n",
    "# print(df)\n",
    "\n",
    "# # run Logistic Regression with optimized hyperparameters\n",
    "# scores = []\n",
    "# score_3 = optimal_logreg(x = got, y = got_target, columns = candidate_dic['top_columns_3'], ran_gridsearch = True, solver = 'newton-cg', C = 2.5, warm_start = False)\n",
    "# scores.append(score_3)\n",
    "# score_4 = optimal_logreg(x = got, y = got_target, columns = candidate_dic['top_columns_4'], ran_gridsearch = True, solver = 'newton-cg', C = 3.0, warm_start = True)\n",
    "# scores.append(score_4)\n",
    "\n",
    "# lr_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= ['top_columns_3', 'top_columns_4'])\n",
    "# lr_score_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134509c5",
   "metadata": {},
   "source": [
    "<h2> Model 2 - KNN Classification </h2>\n",
    "\n",
    "My second approach for solving the problem is a KNN Classification model. \n",
    "\n",
    "<h3> Running KNN Regression</h3>\n",
    "For later use, I wrote a function that runs KNN regression automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb64fc",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KNN Classification\n",
    "def optimal_knn(x,\n",
    "                y,\n",
    "                columns       = candidate_dic['top_columns_3'],\n",
    "                max_neighbors = 20,\n",
    "                show_viz      = False,\n",
    "                standardize   = False):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the training, testing and final score as well\n",
    "as the confusion matrix.\n",
    "PARAMETERS\n",
    "----------\n",
    "x             : Whole explanatory dataset\n",
    "y             : Whole target variable \n",
    "columns       : Columns used in explanatory variable\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"\n",
    "    # slize the columns\n",
    "    x = x[columns]\n",
    "\n",
    "    # creating train and test sets\n",
    "    if standardize:\n",
    "        x_train, x_test, y_train, y_test = test_train_standardize(x, y, standardize = True)\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test = test_train_standardize(x, y)\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    auc_score_list = []\n",
    "    conf_matrix_list = []\n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # creating KNN Classifier and fit train data\n",
    "        knn = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "        knn_fit = knn.fit(x_train, y_train) \n",
    "        \n",
    "        # predict \n",
    "        knn_pred = knn_fit.predict(x_test)\n",
    "           \n",
    "        # recording the training set accuracy\n",
    "        train_score = knn_fit.score(x_train, y_train)\n",
    "        training_accuracy.append(train_score)\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_score = knn_fit.score(x_test, y_test)\n",
    "        test_accuracy.append(test_score)\n",
    "         \n",
    "        # recording the auc score\n",
    "        auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                  y_score = knn_pred).round(decimals = 4)\n",
    "        auc_score_list.append(auc_score)\n",
    "        \n",
    "        # creating confusion matrix\n",
    "        knn_tn, knn_fp, knn_fn, knn_tp = confusion_matrix(y_true = y_test, y_pred = knn_pred).ravel()\n",
    "        knn_conf_matrix = [knn_tn, knn_fp, knn_fn, knn_tp]\n",
    "        conf_matrix_list.append(knn_conf_matrix)\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # index of neighbors with best auc score\n",
    "    index_best = auc_score_list.index(max(auc_score_list))\n",
    "\n",
    "    return [test_accuracy[index_best], training_accuracy[index_best], auc_score_list[index_best], conf_matrix_list[index_best]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16488ed7",
   "metadata": {},
   "source": [
    "<h4> Testing column combinations </h4>\n",
    "First, I run KNN Regression on all columns minus the dropped ones without standardization. That came out with an higher AUC-Score, but a big gap between testing and training.\n",
    "\n",
    "~~~\n",
    "             Test Score  Train Score  AUC Score   Confusion Matrix\n",
    "All Columns    0.861538     0.990862     0.8086  [35, 15, 12, 133]\n",
    "~~~\n",
    "\n",
    "So the model seems to overfit on the chosen columns. Running the KNN-Classifier on the pre-defined columns, I got the following result:\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score   Confusion Matrix\n",
    "top_columns_1    0.789744     0.808110     0.8193   [44, 6, 35, 110]\n",
    "top_columns_2    0.856410     0.840662     0.8117  [36, 14, 14, 131]\n",
    "top_columns_3    0.912821     0.853798     0.8628   [38, 12, 5, 140]\n",
    "top_columns_4    0.897436     0.825814     0.8262   [34, 16, 4, 141]\n",
    "top_columns_5    0.769231     0.700742     0.7990   [43, 7, 38, 107]\n",
    "top_columns_6    0.851282     0.795545     0.8148  [37, 13, 16, 129]\n",
    "~~~\n",
    "\n",
    "Again, column pair 3 does the best job, even though the gap is still a little too big. Knowing that, I ran a couple of Classifications with a new set of columns named <em>knn_dic</em> to check if I can get this score up and the gap down, but it did not seem to make big of a difference, though knn_1 got up the score a little bit.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score   Confusion Matrix\n",
    "top_columns_3    0.912821     0.853798     0.8628   [38, 12, 5, 140]\n",
    "knn_1            0.912821     0.852656     0.8693   [39, 11, 6, 139]\n",
    "knn_2            0.912821     0.849800     0.8628   [38, 12, 5, 140]\n",
    "knn_3            0.912821     0.854940     0.8628   [38, 12, 5, 140]\n",
    "knn_4            0.923077     0.849229     0.8762   [39, 11, 4, 141]\n",
    "knn_5            0.897436     0.825243     0.8262   [34, 16, 4, 141]\n",
    "~~~\n",
    "\n",
    "Last, I ran the classification on my original column dictionary and I was standardizing the columns this time. As we deal with a lot of boolean columns, it did not make the score better.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score   Confusion Matrix\n",
    "top_columns_1    0.774359     0.804112     0.7959   [42, 8, 36, 109]\n",
    "top_columns_2    0.769231     0.797259     0.7924   [42, 8, 37, 108]\n",
    "top_columns_3    0.841026     0.841805     0.8014  [36, 14, 17, 128]\n",
    "top_columns_4    0.897436     0.825814     0.8262   [34, 16, 4, 141]\n",
    "top_columns_5    0.753846     0.695603     0.7886   [43, 7, 41, 104]\n",
    "top_columns_6    0.851282     0.794974     0.8148  [37, 13, 16, 129]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run knn with all columns\n",
    "# knn_all = optimal_knn(got, got_target, got.columns)\n",
    "\n",
    "# # run knn with columns in candidate dic\n",
    "# scores = [knn_all]\n",
    "# for x in candidate_dic:\n",
    "#     score = optimal_knn(x = got, y = got_target, columns = candidate_dic[x])\n",
    "#     scores.append(score)\n",
    "\n",
    "# idx = ['All Columns']\n",
    "# for key in candidate_dic.keys():\n",
    "#     idx.append(key)\n",
    "\n",
    "# knn_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= idx)\n",
    "\n",
    "# # new dictionary changing up column combination 3\n",
    "knn_dic = {\n",
    "        'knn_1' :['popularity', 'age_birth', 'book4_A_Feast_For_Crows', 'book1_A_Game_Of_Thrones', 'culture_Valyrian'],\n",
    "        'knn_2' :['popularity', 'age_birth', 'book4_A_Feast_For_Crows', 'book1_A_Game_Of_Thrones', 'numDeadRelations'],\n",
    "        'knn_3' :['popularity', 'age_birth', 'book4_A_Feast_For_Crows', 'book1_A_Game_Of_Thrones', 'numDeadRelations', 'culture_Valyrian'],\n",
    "        'knn_4' :['popularity', 'age_birth', 'book4_A_Feast_For_Crows', 'book1_A_Game_Of_Thrones', 'numDeadRelations', 'show_only'],\n",
    "        'knn_5' :['age_birth', 'book4_A_Feast_For_Crows', 'book1_A_Game_Of_Thrones']\n",
    "    }\n",
    "\n",
    "# # run knn on that new dictionary\n",
    "# scores = []\n",
    "# for x in knn_dic:\n",
    "#     score = optimal_knn(x = got, y = got_target, columns = knn_dic[x])\n",
    "#     scores.append(score)\n",
    "\n",
    "# knn_score_df_new = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= knn_dic.keys())\n",
    "\n",
    "# # run knn with standardization\n",
    "# scores = []\n",
    "# for x in candidate_dic:\n",
    "#     score = optimal_knn(x = got, y = got_target, columns = candidate_dic[x], standardize=True)\n",
    "#     scores.append(score)\n",
    "    \n",
    "# knn_score_df_stand = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= candidate_dic.keys())\n",
    "\n",
    "\n",
    "# knn_score_df = pd.concat([knn_score_df, knn_score_df_new, knn_score_df_stand])\n",
    "\n",
    "# knn_score_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b7914",
   "metadata": {},
   "source": [
    "<h2> Model 3 - Decision Tree </h2>\n",
    "\n",
    "My third approach for solving the problem is a Decision Tree. \n",
    "\n",
    "<h3> Running Decision Tree</h3>\n",
    "For later use, I wrote a function that runs a Decision Tree automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_dectree(x, \n",
    "                    y, \n",
    "                    columns          = candidate_dic['top_columns_3'],\n",
    "                    ran_gridsearch   = False,\n",
    "                    criterion        = 'gini', \n",
    "                    splitter         = 'best', \n",
    "                    max_depth        = 8, \n",
    "                    min_samples_leaf = 25):\n",
    "    \"\"\"\n",
    "Runs a decision tree on a dataset and target variable. Slizes down the \n",
    "explanatory variable by predifined columns first and solves the decision\n",
    "tree with the predefined parameters after.\n",
    "x                : Whole explanatory dataset\n",
    "y                : Whole target variable \n",
    "columns          : Columns used in explanatory variable\n",
    "ran_gridsearch   : Variable that clarifies, if grid search was run. In that case, the model is fit on the\n",
    "                  whole dataset, else only on the train-dataset. \n",
    "criterion        : criterion used in decision tree\n",
    "splitter         : splitter used in decision tree\n",
    "max_depth        : max depth used in decision tree\n",
    "min_samples_leaf : minimum samples leaf used in decision tree\n",
    "    \"\"\"\n",
    "        \n",
    "    # slize the columns\n",
    "    x = x[columns]\n",
    "    \n",
    "    # INSTANTIATING a classification tree object\n",
    "    full_tree = DecisionTreeClassifier(splitter          = splitter,\n",
    "                                        criterion        = criterion,\n",
    "                                        max_depth        = max_depth,\n",
    "                                        min_samples_leaf = min_samples_leaf,\n",
    "                                        random_state     = 219)\n",
    "    \n",
    "    # creating train and test sets\n",
    "    x_train, x_test, y_train, y_test = test_train_standardize(x, y)\n",
    "    \n",
    "    # FITTING the training data\n",
    "    if ran_gridsearch:\n",
    "        # ran cv gridsearch before, therefor fitting on whole dataset \n",
    "        full_tree_fit = full_tree.fit(x, y)\n",
    "    else:\n",
    "        # no cv before --> fitting on train data\n",
    "        full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING on new data\n",
    "    full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "    # saving scoring data for future use\n",
    "    full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "    full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "    # saving AUC\n",
    "    full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = full_tree_pred).round(4) # auc\n",
    "    \n",
    "    # creating confusion matrix\n",
    "    tree_tn, tree_fp, tree_fn, tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "    tree_conf_matrix = [tree_tn, tree_fp, tree_fn, tree_tp]\n",
    "    \n",
    "#     # look for most important columns\n",
    "#     df = pd.DataFrame(full_tree_fit.feature_importances_, index = x.columns, columns = ['Importance']).sort_values(by = 'Importance', ascending=False)\n",
    "#     print(df.head(n=10))\n",
    "    \n",
    "    return [full_tree_test_score, full_tree_train_score, full_tree_auc_score, tree_conf_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef59fa",
   "metadata": {},
   "source": [
    "<h4> Testing column combinations </h4>\n",
    "First, I run the decision tree on all columns minus the dropped ones. That lead to a good AUC score of 0.83.\n",
    "\n",
    "~~~\n",
    "\n",
    "             Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "All Columns      0.8974       0.8498     0.8328  [35, 15, 5, 140]\n",
    "~~~\n",
    "\n",
    "I then verified which are the most important columns with the following code:\n",
    "\n",
    "~~~\n",
    "df = pd.DataFrame(\n",
    "        full_tree_fit.feature_importances_, \n",
    "        index = x.columns, \n",
    "        columns = ['Importance']).sort_values(by ='Importance', ascending=False)\n",
    "print(df.head(n=10))\n",
    "~~~\n",
    "\n",
    "For the decision tree, these where the most important columns:\n",
    "\n",
    "~~~\n",
    "                            Importance\n",
    "age_birth                     0.528619\n",
    "popularity                    0.164484\n",
    "book4_A_Feast_For_Crows       0.097439\n",
    "total_books_in                0.063694\n",
    "book1_A_Game_Of_Thrones       0.037923\n",
    "book5_A_Dance_with_Dragons    0.036501\n",
    "m_house                       0.016557\n",
    "m_spouse                      0.015028\n",
    "m_title                       0.012088\n",
    "houseSize                     0.011523\n",
    "~~~\n",
    "\n",
    "Running the tree with the top 5 columns already found a better AUC score.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "tree_top5          0.9077       0.8447     0.8593  [38, 12, 6, 139]\n",
    "tree_top8          0.8974       0.8498     0.8328  [35, 15, 5, 140]\n",
    "tree_top10         0.8974       0.8498     0.8328  [35, 15, 5, 140]\n",
    "~~~\n",
    "\n",
    "Last, once again I ran the decision tree on the first pre-defined set of columns:\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "top_columns_1      0.8974       0.8384     0.8197  [33, 17, 3, 142]\n",
    "top_columns_2      0.8923       0.8367     0.8359  [36, 14, 7, 138]\n",
    "top_columns_3      0.9077       0.8429     0.8528  [37, 13, 5, 140]\n",
    "top_columns_4      0.8923       0.8155     0.8162  [33, 17, 4, 141]\n",
    "top_columns_5      0.8564       0.8241     0.7200  [22, 28, 0, 145]\n",
    "top_columns_6      0.8564       0.8258     0.7200  [22, 28, 0, 145]\n",
    "~~~\n",
    "\n",
    "Again, column set 3 seems to be a good fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ddfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run decision tree with all columns\n",
    "# tree_all = optimal_dectree(got, got_target, got.columns)\n",
    "\n",
    "# # run decision tree with columns in candidate dic\n",
    "# scores = [tree_all]\n",
    "# for x in candidate_dic:\n",
    "#     score = optimal_dectree(x = got, y = got_target, columns = candidate_dic[x])\n",
    "#     scores.append(score)\n",
    "\n",
    "# idx = ['All Columns']\n",
    "# for key in candidate_dic.keys():\n",
    "#     idx.append(key)\n",
    "\n",
    "# tree_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= idx)\n",
    "\n",
    "# # new dictionary with most important features\n",
    "tree_dic = {\n",
    "        'tree_top5' : ['age_birth', 'popularity', 'book4_A_Feast_For_Crows', 'total_books_in', 'book1_A_Game_Of_Thrones'],\n",
    "        'tree_top8' : ['age_birth', 'popularity', 'book4_A_Feast_For_Crows', 'total_books_in', 'book1_A_Game_Of_Thrones', 'book5_A_Dance_with_Dragons', 'm_house', 'm_spouse'],\n",
    "        'tree_top10': ['age_birth', 'popularity', 'book4_A_Feast_For_Crows', 'total_books_in', 'book1_A_Game_Of_Thrones', 'book5_A_Dance_with_Dragons', 'm_house', 'm_spouse', 'm_title', 'houseSize']\n",
    "    }\n",
    "\n",
    "# # run decision tree on that new dictionary\n",
    "# scores = []\n",
    "# for x in tree_dic:\n",
    "#     score = optimal_dectree(x = got, y = got_target, columns = tree_dic[x])\n",
    "#     scores.append(score)\n",
    "\n",
    "# tree_score_df_new = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= tree_dic.keys())\n",
    "# tree_score_df = pd.concat([tree_score_df, tree_score_df_new])\n",
    "\n",
    "# tree_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57025c66",
   "metadata": {},
   "source": [
    "<h4> Running Hyperparameter Tuning </h4>\n",
    "For the best models found before, I now run hyperparameter tuning. The candidates tested are <em> tree_top5 </em> and <em> top_columns_3 </em>. It came out with the following results:\n",
    "\n",
    "~~~\n",
    "              splitter  min_samples_leaf  max_depth criterion\n",
    "top_columns_3     best                 4          7   entropy\n",
    "tree_top5         best                 4          7   entropy\n",
    "~~~\n",
    "\n",
    "Running the decision tree with these parameters lowered the AUC score.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "top_columns_3      0.9128       0.8555     0.8431  [35, 15, 2, 143]\n",
    "tree_top5          0.9179       0.8521     0.8466  [35, 15, 1, 144]\n",
    "~~~\n",
    "\n",
    "However, changing the max depth to 8 instead of 7 and leaving the rest as found in the randomn search, highered the AUC score to over 0.86 for both of the sets. Looking at the confusion matrix, the <em> top_columns_3 </em> did a slightly better job.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "top_columns_3      0.9231       0.8549     0.8631  [37, 13, 2, 143]\n",
    "tree_top5          0.9128       0.8578     0.8628  [38, 12, 5, 140]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_dectree(x, \n",
    "                   y,\n",
    "                   columns = candidate_dic['top_columns_3']):\n",
    "    \"\"\"\n",
    "Runs a cross validation on a decision tree. Outputs the best parameters\n",
    "found in the search.\n",
    "Parameters:\n",
    "x       : Whole explanatory dataset\n",
    "y       : Whole target variable\n",
    "columns : columns used in explanatory variable\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # slize the columns\n",
    "    x = x[columns] \n",
    "    \n",
    "    # declaring a hyperparameter space\n",
    "    criterion_range = ['gini', 'entropy']\n",
    "    splitter_range  = ['best', 'random']\n",
    "    depth_range     = np.arange(1, 8, 1)\n",
    "    leaf_range      = np.arange(1, 100, 1)\n",
    "\n",
    "    # creating a hyperparameter grid\n",
    "    param_grid = {'criterion'        : criterion_range,\n",
    "                  'splitter'         : splitter_range,\n",
    "                  'max_depth'        : depth_range,\n",
    "                  'min_samples_leaf' : leaf_range}\n",
    " \n",
    "    # INSTANTIATING the model object without hyperparameters\n",
    "    tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "    # RandomizedSearchCV object\n",
    "    tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "                                       param_distributions   = param_grid,\n",
    "                                       cv                    = 3,\n",
    "                                       n_iter                = 1000,\n",
    "                                       random_state          = 219,\n",
    "                                       scoring = make_scorer(roc_auc_score,\n",
    "                                                 needs_threshold = False))\n",
    "\n",
    "    # FITTING to the FULL DATASET (due to cross-validation)\n",
    "    tuned_tree_cv.fit(x, y)\n",
    "    \n",
    "    return tuned_tree_cv.best_params_\n",
    "    \n",
    "# # find optimal hyperparameters for set 3 and tree_top5\n",
    "# opt_1 = run_cv_dectree(got, got_target, columns = candidate_dic['top_columns_3'])\n",
    "# opt_2 = run_cv_dectree(got, got_target, columns = tree_dic['tree_top5'])\n",
    "\n",
    "# df = pd.DataFrame([opt_1, opt_2], index = ['top_columns_3', 'tree_top5'])\n",
    "# print(df)\n",
    "\n",
    "# # run decision tree with optimized hyperparameters\n",
    "# scores = []\n",
    "# score_3 = optimal_dectree(x = got, y = got_target, columns = candidate_dic['top_columns_3'], ran_gridsearch = True, criterion = 'entropy', splitter = 'best', max_depth = 8, min_samples_leaf = 4)\n",
    "# scores.append(score_3)\n",
    "# score_top5 = optimal_dectree(x = got, y = got_target, columns = tree_dic['tree_top5'], ran_gridsearch = True, criterion = 'entropy', splitter = 'best', max_depth = 8, min_samples_leaf = 4)\n",
    "# scores.append(score_top5)\n",
    "\n",
    "# tree_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= ['top_columns_3', 'tree_top5'])\n",
    "# print(tree_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d10ff",
   "metadata": {},
   "source": [
    "<h2> Model 4 - Random Forest </h2>\n",
    "\n",
    "My fourth approach for solving the problem is a Random Forest. \n",
    "\n",
    "<h3> Running Random Forrest</h3>\n",
    "For later use, I wrote a function that runs a Random Forest automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_ranfor( x, \n",
    "                    y, \n",
    "                    columns          = candidate_dic['top_columns_3'],\n",
    "                    ran_gridsearch   = False,\n",
    "                    n_estimators     = 100,\n",
    "                    criterion        = 'gini',\n",
    "                    bootstrap        = True, \n",
    "                    max_depth        = 8, \n",
    "                    min_samples_leaf = 25,\n",
    "                    warm_start       = False):\n",
    "    \"\"\"\n",
    "Runs a decision tree on a dataset and target variable. Slizes down the \n",
    "explanatory variable by predifined columns first and solves the decision\n",
    "tree with the predefined parameters after.\n",
    "x                : Whole explanatory dataset\n",
    "y                : Whole target variable \n",
    "columns          : Columns used in explanatory variable\n",
    "ran_gridsearch   : Variable that clarifies, if grid search was run. In that case, the model is fit on the\n",
    "                  whole dataset, else only on the train-dataset. \n",
    "n_estimators     : number of estimators used in decision tree\n",
    "criterion        : criterion used in decision tree\n",
    "bootstrap        : bootstrap used in decision tree\n",
    "max_depth        : max depth used in decision tree\n",
    "min_samples_leaf : minimum samples leaf used in decision tree\n",
    "warm_start       : whether true or false, warm start used in decision tree\n",
    "    \"\"\"\n",
    "        \n",
    "    # slize the columns\n",
    "    x = x[columns]\n",
    "    \n",
    "    # INSTANTIATING a classification tree object\n",
    "    rf  = RandomForestClassifier(n_estimators    = n_estimators,\n",
    "                                criterion        = criterion,\n",
    "                                max_depth        = max_depth,\n",
    "                                min_samples_leaf = min_samples_leaf,\n",
    "                                bootstrap        = bootstrap,\n",
    "                                warm_start       = warm_start,\n",
    "                                random_state     = 219)\n",
    "    \n",
    "    # creating train and test sets\n",
    "    x_train, x_test, y_train, y_test = test_train_standardize(x, y)\n",
    "    \n",
    "    # FITTING the training data\n",
    "    if ran_gridsearch:\n",
    "        # ran cv gridsearch before, therefor fitting on whole dataset \n",
    "        rf_fit = rf.fit(x, y)\n",
    "    else:\n",
    "        # no cv before --> fitting on train data\n",
    "        rf_fit = rf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING on new data\n",
    "    rf_pred = rf_fit.predict(x_test)\n",
    "\n",
    "\n",
    "    # saving scoring data for future use\n",
    "    rf_train_score = rf_fit.score(x_train, y_train).round(4) # accuracy\n",
    "    rf_test_score  = rf_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "    # saving AUC\n",
    "    rf_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = rf_pred).round(4) # auc\n",
    "    \n",
    "    # creating confusion matrix\n",
    "    rf_tn, rf_fp, rf_fn, rf_tp = confusion_matrix(y_true = y_test, y_pred = rf_pred).ravel()\n",
    "    rf_conf_matrix = [rf_tn, rf_fp, rf_fn, rf_tp]\n",
    "    \n",
    "#     # look for most important columns\n",
    "#     df = pd.DataFrame(rf_fit.feature_importances_, index = x.columns, columns = ['Importance']).sort_values(by = 'Importance', ascending=False)\n",
    "#     print(df.head(n=15))\n",
    "    \n",
    "    return [rf_test_score, rf_train_score, rf_auc_score, rf_conf_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa467430",
   "metadata": {},
   "source": [
    "<h4> Testing column combinations </h4>\n",
    "First, I run the random forest on all columns minus the dropped ones. That lead to bad a AUC score of 0.50.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "All Columns        0.7436       0.7470     0.5000   [0, 50, 0, 145]\n",
    "~~~\n",
    "\n",
    "As for the decision tree, I verified the most important columns:\n",
    "\n",
    "~~~\n",
    "                         Importance\n",
    "popularity                 0.107835\n",
    "age_birth                  0.106666\n",
    "dateOfBirth                0.101263\n",
    "total_books_in             0.081695\n",
    "book4_A_Feast_For_Crows    0.076046\n",
    "show_only                  0.056806\n",
    "numDeadRelations           0.047509\n",
    "house_House_Frey           0.046862\n",
    "houseSize                  0.044695\n",
    "house_House_Targaryen      0.040999\n",
    "book1_A_Game_Of_Thrones    0.040636\n",
    "house_Night's_Watch        0.034534\n",
    "age                        0.028214\n",
    "culture_Ironborn           0.025178\n",
    "book2_A_Clash_Of_Kings     0.022260\n",
    "~~~\n",
    "\n",
    "Running the random forest with the top 5 columns once again found a better AUC score.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "rf_top5            0.9077       0.8458     0.8331  [34, 16, 2, 143]\n",
    "rf_top8            0.8410       0.8104     0.6900  [19, 31, 0, 145]\n",
    "rf_top10           0.8769       0.8252     0.7666  [27, 23, 1, 144]\n",
    "rf_top15           0.8615       0.8150     0.7366  [24, 26, 1, 144]\n",
    "~~~\n",
    "\n",
    "\n",
    "Last, once again I ran the forest on the first pre-defined set of columns:\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "top_columns_1      0.8667       0.8281     0.7662  [28, 22, 4, 141]\n",
    "top_columns_2      0.8718       0.8441     0.7631  [27, 23, 2, 143]\n",
    "top_columns_3      0.9026       0.8412     0.8297  [34, 16, 3, 142]\n",
    "top_columns_4      0.8462       0.8104     0.7000  [20, 30, 0, 145]\n",
    "top_columns_5      0.8256       0.8081     0.6600  [16, 34, 0, 145]\n",
    "top_columns_6      0.8462       0.8104     0.7000  [20, 30, 0, 145]\n",
    "~~~\n",
    "\n",
    "Our favourite column set 3 does a good job again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run random forest with all columns\n",
    "# rf_all = optimal_ranfor(got, got_target, got.columns)\n",
    "\n",
    "# # run random forest with columns in candidate dic\n",
    "# scores = [rf_all]\n",
    "# for x in candidate_dic:\n",
    "#     score = optimal_ranfor(x = got, y = got_target, columns = candidate_dic[x])\n",
    "#     scores.append(score)\n",
    "\n",
    "# idx = ['All Columns']\n",
    "# for key in candidate_dic.keys():\n",
    "#     idx.append(key)\n",
    "\n",
    "# rf_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= idx)\n",
    "\n",
    "# # new dictionary with most important features\n",
    "rf_dic = {\n",
    "        'rf_top5' : ['popularity', 'age_birth', 'dateOfBirth', 'total_books_in', 'book4_A_Feast_For_Crows'],\n",
    "        'rf_top8' : ['popularity', 'age_birth', 'dateOfBirth', 'total_books_in', 'book4_A_Feast_For_Crows', 'show_only', 'numDeadRelations', 'house_House_Frey'],\n",
    "        'rf_top10': ['popularity', 'age_birth', 'dateOfBirth', 'total_books_in', 'book4_A_Feast_For_Crows', 'show_only', 'numDeadRelations', 'house_House_Frey', 'houseSize', 'house_House_Targaryen'],\n",
    "        'rf_top15': ['popularity', 'age_birth', 'dateOfBirth', 'total_books_in', 'book4_A_Feast_For_Crows', 'show_only', 'numDeadRelations', 'house_House_Frey', 'houseSize', 'house_House_Targaryen', 'book1_A_Game_Of_Thrones', \"house_Night's_Watch\", 'age', 'culture_Ironborn', 'book2_A_Clash_Of_Kings'],\n",
    "    \n",
    "    }\n",
    "\n",
    "# # run random forest on that new dictionary\n",
    "# scores = []\n",
    "# for x in rf_dic:\n",
    "#     score = optimal_ranfor(x = got, y = got_target, columns = rf_dic[x])\n",
    "#     scores.append(score)\n",
    "\n",
    "# rf_score_df_new = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= rf_dic.keys())\n",
    "# rf_score_df = pd.concat([rf_score_df, rf_score_df_new])\n",
    "\n",
    "# rf_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb101fa9",
   "metadata": {},
   "source": [
    "<h4> Running Hyperparameter Tuning </h4>\n",
    "For the best models found before, I now run hyperparameter tuning. The candidates tested are <em> tree_top5 </em> and <em> top_columns_3 </em>. It came out with the following results:\n",
    "\n",
    "~~~\n",
    "               warm_start  n_estimators  min_samples_leaf  max_depth  criterion  bootstrap \n",
    "top_columns_3        True           600                 1          6    entropy      False  \n",
    "tree_top5            True           100                 1          7       gini      False         \n",
    "~~~\n",
    "\n",
    "Running the decision tree with the hyperparameters changed leads to a higher AUC score, especially in the <em> tree_top5 </em> columns.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "top_columns_3      0.9179       0.8521     0.8531  [36, 14, 2, 143]\n",
    "tree_top5          0.9231       0.8624     0.8697  [38, 12, 3, 142]\n",
    "~~~\n",
    "\n",
    "Therefor, running the random forest with the <em> tree_top5 </em> columns is the best option here. However, changing the max depth to 8 again, got the score up even more.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "tree_top5          0.9333       0.8664     0.8897  [40, 10, 3, 142]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_ranfor( x, \n",
    "                   y,\n",
    "                   columns = candidate_dic['top_columns_3']):\n",
    "    \"\"\"\n",
    "Runs a cross validation on a random forest. Outputs the best parameters\n",
    "found in the search.\n",
    "Parameters:\n",
    "x       : Whole explanatory dataset\n",
    "y       : Whole target variable\n",
    "columns : columns used in explanatory variable\n",
    "    \"\"\"\n",
    "    # slizing the columns\n",
    "    x = x[columns]\n",
    "    \n",
    "    # declaring a hyperparameter space\n",
    "    estimator_range  = np.arange(100, 1100, 250)\n",
    "    leaf_range       = np.arange(1, 31, 10)\n",
    "    depth_range     = np.arange(1, 8, 1)\n",
    "    criterion_range  = ['gini', 'entropy']\n",
    "    bootstrap_range  = [True, False]\n",
    "    warm_start_range = [True, False]\n",
    "\n",
    "\n",
    "    # creating a hyperparameter grid\n",
    "    param_grid = {'n_estimators'     : estimator_range,\n",
    "                  'min_samples_leaf' : leaf_range,\n",
    "                  'max_depth'        : depth_range,\n",
    "                  'criterion'        : criterion_range,\n",
    "                  'bootstrap'        : bootstrap_range,\n",
    "                  'warm_start'       : warm_start_range}\n",
    " \n",
    "    # INSTANTIATING the model object without hyperparameters\n",
    "    forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "    # GridSearchCV object\n",
    "    forest_cv = RandomizedSearchCV(estimator           = forest_grid,\n",
    "                                   param_distributions = param_grid,\n",
    "                                   cv                  = 3,\n",
    "                                   n_iter              = 1000,\n",
    "                                   scoring             = make_scorer(roc_auc_score,\n",
    "                                                needs_threshold = False))\n",
    "\n",
    "    # FITTING to the FULL DATASET (due to cross-validation)\n",
    "    forest_cv.fit(x, y)\n",
    "    \n",
    "    return forest_cv.best_params_\n",
    "    \n",
    "# # find optimal hyperparameters for set 3 and rf_top5\n",
    "# opt_1 = run_cv_ranfor(got, got_target, columns = candidate_dic['top_columns_3'])\n",
    "# opt_2 = run_cv_ranfor(got, got_target, columns = rf_dic['rf_top5'])\n",
    "\n",
    "# df = pd.DataFrame([opt_1, opt_2], index = ['top_columns_3', 'tree_top5'])\n",
    "# print(df)\n",
    "\n",
    "# # run random forest with optimized hyperparameters\n",
    "# scores = []\n",
    "# score_3 = optimal_ranfor(x = got, y = got_target, columns = candidate_dic['top_columns_3'], ran_gridsearch = True,\n",
    "#                         warm_start = True, n_estimators = 600, min_samples_leaf = 1, max_depth = 6, criterion = 'entropy', bootstrap = False)\n",
    "# scores.append(score_3)\n",
    "# score_top5 = optimal_ranfor(x = got, y = got_target, columns = tree_dic['tree_top5'], ran_gridsearch = True,\n",
    "#                             warm_start = True, n_estimators = 100, min_samples_leaf = 1, max_depth = 8, criterion = 'gini', bootstrap = False)\n",
    "# scores.append(score_top5)\n",
    "\n",
    "# rf_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= ['top_columns_3', 'tree_top5'])\n",
    "# print(rf_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692352b",
   "metadata": {},
   "source": [
    "<h2> Model 5 - Gradient Boosted Machine </h2>\n",
    "\n",
    "My last approach for solving the problem is a GBM. \n",
    "\n",
    "<h3> Running GBM </h3>\n",
    "Once again, I put running the GBM model in a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_gbm(x, \n",
    "                y, \n",
    "                columns          = candidate_dic['top_columns_3'],\n",
    "                ran_gridsearch   = False,\n",
    "                learning_rate    = 0.1,\n",
    "                n_estimators     = 100,\n",
    "                max_depth        = 3,\n",
    "                warm_start       = False):\n",
    "    \"\"\"\n",
    "Runs a gbm on a dataset and target variable. Slizes down the \n",
    "explanatory variable by predifined columns first and solves the decision\n",
    "tree with the predefined parameters after.\n",
    "x                : Whole explanatory dataset\n",
    "y                : Whole target variable \n",
    "columns          : Columns used in explanatory variable\n",
    "ran_gridsearch   : Variable that clarifies, if grid search was run. In that case, the model is fit on the\n",
    "                  whole dataset, else only on the train-dataset. \n",
    "learning_rate    : learning rate used in gbm\n",
    "n_estimators     : number of estimators used in gbm\n",
    "max_depth        : max depth used in gbm\n",
    "warm_start       : whether true or false, warm start used in gbm\n",
    "    \"\"\"\n",
    "        \n",
    "    # slize the columns\n",
    "    x = x[columns]\n",
    "    \n",
    "    # INSTANTIATING the model object without hyperparameters\n",
    "    gbm = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                     learning_rate = learning_rate,\n",
    "                                     n_estimators  = n_estimators,\n",
    "                                     criterion     = 'friedman_mse',\n",
    "                                     max_depth     = max_depth,\n",
    "                                     warm_start    = warm_start,\n",
    "                                     random_state  = 219)\n",
    "    \n",
    "    # creating train and test sets\n",
    "    x_train, x_test, y_train, y_test = test_train_standardize(x, y)\n",
    "    \n",
    "    # FITTING the training data\n",
    "    if ran_gridsearch:\n",
    "        # ran cv gridsearch before, therefor fitting on whole dataset \n",
    "        gbm_fit = gbm.fit(x, y)\n",
    "    else:\n",
    "        # no cv before --> fitting on train data\n",
    "        gbm_fit = gbm.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # PREDICTING on new data\n",
    "    gbm_pred = gbm_fit.predict(x_test)\n",
    "\n",
    "\n",
    "    # saving scoring data for future use\n",
    "    gbm_train_score = gbm_fit.score(x_train, y_train).round(4) # accuracy\n",
    "    gbm_test_score  = gbm_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "    # saving AUC\n",
    "    gbm_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                    y_score = gbm_pred).round(4) # auc\n",
    "    \n",
    "    # creating confusion matrix\n",
    "    gbm_tn, gbm_fp, gbm_fn, gbm_tp = confusion_matrix(y_true = y_test, y_pred = gbm_pred).ravel()\n",
    "    gbm_conf_matrix = [gbm_tn, gbm_fp, gbm_fn, gbm_tp]\n",
    "    \n",
    "    return [gbm_test_score, gbm_train_score, gbm_auc_score, gbm_conf_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd186e7",
   "metadata": {},
   "source": [
    "<h4> Testing column combinations </h4>\n",
    "First, I run the GBM on all columns minus the dropped ones and the pre defined set of columns:\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "All Columns        0.8974       0.8749     0.8131  [32, 18, 2, 143]\n",
    "top_columns_1      0.8769       0.8498     0.7797  [29, 21, 3, 142]\n",
    "top_columns_2      0.8718       0.8561     0.7631  [27, 23, 2, 143]\n",
    "top_columns_3      0.9026       0.8584     0.8231  [33, 17, 2, 143]\n",
    "top_columns_4      0.8564       0.8258     0.7200  [22, 28, 0, 145]\n",
    "top_columns_5      0.8718       0.8350     0.7500  [25, 25, 0, 145]\n",
    "top_columns_6      0.8615       0.8338     0.7300  [23, 27, 0, 145]\n",
    "~~~\n",
    "\n",
    "And once again, set 3 is our prefered model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run gbm with all columns\n",
    "# gbm_all = optimal_gbm(got, got_target, got.columns)\n",
    "\n",
    "# # run gbm with columns in candidate dic\n",
    "# scores = [gbm_all]\n",
    "# for x in candidate_dic:\n",
    "#     score = optimal_gbm(x = got, y = got_target, columns = candidate_dic[x])\n",
    "#     scores.append(score)\n",
    "\n",
    "# idx = ['All Columns']\n",
    "# for key in candidate_dic.keys():\n",
    "#     idx.append(key)\n",
    "\n",
    "# gbm_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= idx)\n",
    "\n",
    "# gbm_score_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52127b9d",
   "metadata": {},
   "source": [
    "<h4> Running Hyperparameter Tuning </h4>\n",
    "The last exercise of this assignment is running hyperparameter tuning on GBM. I only optimize column set 3 in that case.\n",
    "\n",
    "~~~\n",
    "               warm_start  n_estimators  max_depth  learning_rate\n",
    "top_columns_3       False           100          4            0.6       \n",
    "~~~\n",
    "\n",
    "After tuning the parameters, the score got up significantly.\n",
    "\n",
    "~~~\n",
    "               Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "top_columns_3      0.9333       0.8766     0.8831  [39, 11, 2, 143]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7726e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_gbm( x, \n",
    "                y,\n",
    "                columns = candidate_dic['top_columns_3']):\n",
    "    \"\"\"\n",
    "Runs a cross validation on GBM. Outputs the best parameters\n",
    "found in the search.\n",
    "Parameters:\n",
    "x       : Whole explanatory dataset\n",
    "y       : Whole target variable\n",
    "columns : columns used in explanatory variable\n",
    "    \"\"\"\n",
    "    # slizing the columns\n",
    "    x = x[columns]\n",
    "    \n",
    "    # declaring a hyperparameter space\n",
    "    learn_range        = np.arange(0.1, 2.2, 0.5)\n",
    "    estimator_range    = np.arange(100, 501, 25)\n",
    "    depth_range        = np.arange(2, 8, 2)\n",
    "    warm_start_range   = [True, False]\n",
    "\n",
    "\n",
    "    # creating a hyperparameter grid\n",
    "    param_grid = {'learning_rate' : learn_range,\n",
    "                  'max_depth'     : depth_range,\n",
    "                  'n_estimators'  : estimator_range,\n",
    "                  'warm_start'    : warm_start_range}\n",
    "\n",
    "    # INSTANTIATING the model object without hyperparameters\n",
    "    gbm_grid = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "    # GridSearchCV object\n",
    "    gbm_cv = RandomizedSearchCV(estimator          = gbm_grid,\n",
    "                               param_distributions = param_grid,\n",
    "                               cv                  = 3,\n",
    "                               n_iter              = 500,\n",
    "                               random_state        = 219,\n",
    "                               scoring             = make_scorer(roc_auc_score,\n",
    "                                                     needs_threshold = False))\n",
    "\n",
    "    # FITTING to the FULL DATASET (due to cross-validation)\n",
    "    gbm_cv.fit(x, y)\n",
    "    \n",
    "    return gbm_cv.best_params_\n",
    "    \n",
    "# # find optimal hyperparameters for set 3 and rf_top5\n",
    "# opt_1 = run_cv_gbm(got, got_target, columns = candidate_dic['top_columns_3'])\n",
    "\n",
    "# df = pd.DataFrame([opt_1], index = ['top_columns_3'])\n",
    "# print(df)\n",
    "\n",
    "# # run gbm with optimized hyperparameters\n",
    "# scores = []\n",
    "# score_3 = optimal_gbm(x = got, y = got_target, columns = candidate_dic['top_columns_3'], ran_gridsearch = True,\n",
    "#                      warm_start = False, n_estimators = 100, max_depth = 4, learning_rate = 0.6)\n",
    "# scores.append(score_3)\n",
    "\n",
    "\n",
    "# gbm_score_df = pd.DataFrame(scores, columns = ['Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'], \n",
    "#                            index= ['top_columns_3'])\n",
    "# print(gbm_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812dae34",
   "metadata": {},
   "source": [
    "<h2> Bringing it all together </h2>\n",
    "\n",
    "Here comes the interesting part, comparing all the models. For every model type, I collected the one with the highest score. Looking at the final outcome, the random forest scored the highest AUC score. However, looking at the test-train gap, I would choose the GBM-model. In predicted false positives and negatives, the two models are almost the same, so I choose the GBM-model as final model.\n",
    "\n",
    "\n",
    "~~~\n",
    "            Model Name  Test Score  Train Score  AUC Score  Confusion Matrix\n",
    "3        Random Forest    0.933300     0.866400     0.8897  [40, 10, 3, 142]\n",
    "4                  GBM    0.933300     0.876600     0.8831  [39, 11, 2, 143]\n",
    "1                  KNN    0.912821     0.852656     0.8693  [39, 11, 6, 139]\n",
    "2        Decision Tree    0.923100     0.854900     0.8631  [37, 13, 2, 143]\n",
    "0  Logistic Regression    0.851300     0.800700     0.7428  [26, 24, 5, 140]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "model_performance = pd.DataFrame(columns = ['Model Name', 'Test Score', 'Train Score', 'AUC Score', 'Confusion Matrix'])\n",
    "\n",
    "# best logistic regression model\n",
    "logreg = optimal_logreg(x = got, y = got_target, columns = candidate_dic['top_columns_3'], ran_gridsearch = True, solver = 'newton-cg', C = 2.5, warm_start = False)\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append({\n",
    "                        'Model Name': 'Logistic Regression',\n",
    "                        'Test Score': logreg[0],\n",
    "                        'Train Score': logreg[1],\n",
    "                        'AUC Score': logreg[2],\n",
    "                        'Confusion Matrix': logreg[3],\n",
    "                    },\n",
    "                           ignore_index = True)\n",
    "\n",
    "# best KNN model\n",
    "knn = optimal_knn(x = got, y = got_target, columns = knn_dic['knn_1'])\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append({\n",
    "                        'Model Name': 'KNN',\n",
    "                        'Test Score': knn[0],\n",
    "                        'Train Score': knn[1],\n",
    "                        'AUC Score': knn[2],\n",
    "                        'Confusion Matrix': knn[3],\n",
    "                    },\n",
    "                           ignore_index = True)\n",
    "\n",
    "# best tree model\n",
    "tree = optimal_dectree(x = got, y = got_target, columns = candidate_dic['top_columns_3'], ran_gridsearch = True, criterion = 'entropy', splitter = 'best', max_depth = 8, min_samples_leaf = 4)\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append({\n",
    "                        'Model Name': 'Decision Tree',\n",
    "                        'Test Score': tree[0],\n",
    "                        'Train Score': tree[1],\n",
    "                        'AUC Score': tree[2],\n",
    "                        'Confusion Matrix': tree[3],\n",
    "                    },\n",
    "                           ignore_index = True)\n",
    "\n",
    "\n",
    "# best forest model\n",
    "rf = optimal_ranfor(x = got, y = got_target, columns = tree_dic['tree_top5'], ran_gridsearch = True,\n",
    "                    warm_start = True, n_estimators = 100, min_samples_leaf = 1, max_depth = 8, criterion = 'gini', bootstrap = False)\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append({\n",
    "                        'Model Name': 'Random Forest',\n",
    "                        'Test Score': rf[0],\n",
    "                        'Train Score': rf[1],\n",
    "                        'AUC Score': rf[2],\n",
    "                        'Confusion Matrix': rf[3],\n",
    "                    },\n",
    "                           ignore_index = True)\n",
    "\n",
    "\n",
    "# best gbm model\n",
    "gbm = optimal_gbm(x = got, y = got_target, columns = candidate_dic['top_columns_3'], ran_gridsearch = True,\n",
    "                  warm_start = False, n_estimators = 100, max_depth = 4, learning_rate = 0.6)\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append({\n",
    "                        'Model Name': 'GBM',\n",
    "                        'Test Score': gbm[0],\n",
    "                        'Train Score': gbm[1],\n",
    "                        'AUC Score': gbm[2],\n",
    "                        'Confusion Matrix': gbm[3],\n",
    "                    },\n",
    "                           ignore_index = True)\n",
    "\n",
    "# checking the results\n",
    "# print(model_performance.sort_values(by = 'AUC Score', ascending = False))\n",
    "\n",
    "print(f\"\"\"\n",
    "My final model chosen is the GBM-model with the following scores:\n",
    "\n",
    "\n",
    "Model    Test Score      Train Score      Train-Test-Gap      AUC Score       Confusion Matrix\n",
    "-----    -----------     -----------      --------------      ---------      -----------------\n",
    "*GBM*    {gbm[0]}           {gbm[1]}          {(gbm[0]-gbm[1]).round(decimals = 4)}              {gbm[2]}         {gbm[3]}\n",
    " RF      {rf[0]}           {rf[1]}          {(rf[0]-rf[1]).round(decimals = 4)}              {rf[2]}         {rf[3]}\n",
    " KNN     {knn[0].round(decimals=4)}           {knn[1].round(decimals=4)}          {(knn[0]-knn[1]).round(decimals = 4)}              {knn[2]}         {knn[3]}\n",
    " Tree    {tree[0]}           {tree[1]}          {(tree[0]-tree[1]).round(decimals = 4)}              {tree[2]}         {tree[3]}\n",
    " LR      {logreg[0]}           {logreg[1]}          {(logreg[0]-logreg[1]).round(decimals = 4)}              {logreg[2]}         {logreg[3]}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
